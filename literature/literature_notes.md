**Statistical Rituals: The Replication Delusion and How We Got There**, [Gigerenzer (2018)](https://doi.org/10.1177/2515245918771329) 
*The null ritual*
"A ritual is a collective or solemn ceremony consisting of actions performed in a prescribed order. It typically includes the following elements. 
- sacred numbers or colours (5% $\alpha$ level)
- repetition of the same action (checking behaviour, testing hypotheses)
- fear about being punished when one stops performing these actions (sanctions by editors or advisors)
- wishful thinking and delusions (about the meaning of the p-value

*Probability of replication*
>For instance, a result with a p value of .05 means that if the null hypothesis—including all assumptions made by the underlying model—is true, the probability of obtaining such a result or a more extreme one is 5%. The p value does not tell us much else. Specifically, a p value of .05 does not imply that the probability that the result can be replicated is 95%.

*The Replication Delusion*
>The belief that an obtained p value implies a probability of 1 – p that an exact replication of the same experiment would lead to a significant result. [Rosenthal, 1993](https://taylorfrancis.com/chapters/edit/10.4324/9781315799582-24/cumulating-evidence-robert-rosenthal?context=ubx&refId=769cfe71-b624-44b1-ac49-c4481f6aa497) 

*Better than a coin flip?*
>A meta-analysis of 44 reviews published in the social and behavioral sciences, beginning with Cohen’s 1962 study, found that power had not increased over half a century (Smaldino & McElreath, 2016). Instead, the average power had remained consistently low, and the mean power for detecting a small sized effect (Cohen’s d = 0.2) was 24%, assuming α = .05.


**History of Replication Failures in Psychology, *in Avoiding Questionable Research Practices in Applied Psychology***, [Whitt et al. (2022)](https://link.springer.com/book/10.1007/978-3-031-04968-2) 



**Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size**, [Kühberger et al. (2014)](https://doi.org/10.1371/journal.pone.0105825) 
*Background*
>The _p_ value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias.

*Results*
>We found a negative correlation of r = −.45 (95% CI: −.53; −.35) between effect size and sample size. In addition, we found an inordinately high number of _p_ values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings.

*Conclusion*
>The negative correlation between effect size and samples size, and the biased distribution of _p_ values indicate pervasive publication bias in the entire field of psychology.




**Addressing the "Replication Crisis": Using Original Studies to Design Replication Studies with Appropriate Statistical Power**, [Anderson & Maxwell (2017)](https://doi.org/10.1080/00273171.2017.1289361)
*Why is the most likely power of the replication study so much less than the intended power?*
>One potential explanation is that the effect size reported in the original study is the result of p-hacking, the "garden of forking paths" (Gelman & Loken, 2014), or hypothesizing after results are known (HARKing; Kerr, 1998). 

*Nonlinear association between effect size and power*
>The power loss from using an overestimate of effect size is generally larger than the power gain from using an underestimate that differs from the population value by the same magnitude as the overestimate (Maxwell et al., 2015). 

*Short Conclusion of Results*
>The results of this study indicate that common methods of planning the sample size for replication studies provide inadequate estimates of the actual power achieved. This general pattern is unsurprising. However, more noteworthy is that the magnitude of these overestimates can be quite substantial.

